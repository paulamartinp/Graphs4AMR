{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random, os, json\n",
    "# Configurar variables de entorno\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # \"-1\" significa deshabilitar todas las GPUs\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateKPI(parameter):\n",
    "    \"\"\"\n",
    "    This function calculate the mean and deviation of a set of values of\n",
    "    a given performance indicator.\n",
    "    \n",
    "    Returns: Mean and std (float)\n",
    "    \"\"\"\n",
    "    mean = round(np.mean(parameter)*100, 2)\n",
    "    deviation = round(np.sqrt(np.sum(np.power(parameter - np.mean(parameter), 2) / len(parameter)))*100, 2)\n",
    "    return mean, deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_keras(seed=42):\n",
    "    \"\"\"Function to ensure that results from Keras models\n",
    "    are consistent and reproducible across different runs\"\"\"\n",
    "    \n",
    "    K.clear_session()\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    \"\"\"\n",
    "    Builds a GRU model with optional regularization.\n",
    "    \"\"\"\n",
    "    reg_type = hyperparameters.get('regularizer', {}).get('type', None)\n",
    "    reg_value = hyperparameters.get('regularizer', {}).get('value', 0.0)\n",
    "\n",
    "    if reg_type == 'l1':\n",
    "        regularizer = tf.keras.regularizers.l1(reg_value)\n",
    "    elif reg_type == 'l2':\n",
    "        regularizer = tf.keras.regularizers.l2(reg_value)\n",
    "    elif reg_type == 'l1_l2':\n",
    "        regularizer = tf.keras.regularizers.l1_l2(reg_value)\n",
    "    else:\n",
    "        regularizer = None\n",
    "\n",
    "    input_layer = tf.keras.layers.Input(shape=(hyperparameters[\"n_time_steps\"], hyperparameters[\"layers\"][0]))\n",
    "    masked = tf.keras.layers.Masking(mask_value=hyperparameters['mask_value'])(input_layer)\n",
    "\n",
    "    gru = tf.keras.layers.GRU(\n",
    "        hyperparameters[\"layers\"][1],\n",
    "        dropout=hyperparameters['dropout'],\n",
    "        return_sequences=False,\n",
    "        activation=hyperparameters['activation'],\n",
    "        kernel_regularizer=regularizer,\n",
    "        bias_regularizer=regularizer,\n",
    "        use_bias=True\n",
    "    )(masked)\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_regularizer=regularizer,\n",
    "        bias_regularizer=regularizer\n",
    "    )(gru)\n",
    "\n",
    "    model = tf.keras.Model(input_layer, [output])\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"lr_scheduler\"]),\n",
    "        metrics=['accuracy', \"AUC\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(X_train, X_val, y_train, y_val, hyperparameters, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the built LSTM model based on the provided data and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        - X_train, X_val, y_train, y_val: numpy.ndarray. Training (T) and Validation (V) data labels.\n",
    "        - sample_weights_train, sample_weights_val: numpy.ndarray. Weights for the T and V data to handle class imbalance.\n",
    "        - hyperparameters: Dictionary containing the hyperparameters.\n",
    "        - seed: Integer seed for reproducibility.\n",
    "    Returns:\n",
    "        - model: A tf.keras.Model with the trained model.\n",
    "        - hist:  The training history.\n",
    "        - earlystopping: The early stopping callback.\n",
    "    \"\"\"\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    n_epochs_max = hyperparameters['n_epochs_max']    \n",
    "\n",
    "    model = None\n",
    "    model = build_model(hyperparameters)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  min_delta=hyperparameters[\"mindelta\"],\n",
    "                                                  patience=hyperparameters[\"patience\"],\n",
    "                                                  restore_best_weights=True,\n",
    "                                                  mode=\"min\")\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     callbacks=[earlystopping], batch_size=batch_size, epochs=n_epochs_max,\n",
    "                     verbose=hyperparameters['verbose'])\n",
    "    \n",
    "    return model, hist, earlystopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combination(k, l, m, b, r, hyperparameters, dropout, layers, lr_scheduler, activation, regularizer, seed, split, norm, n_time_steps):\n",
    "    hyperparameters_copy = hyperparameters.copy()\n",
    "    hyperparameters_copy['dropout'] = dropout[k]\n",
    "    hyperparameters_copy['layers'] = layers[l]\n",
    "    hyperparameters_copy['lr_scheduler'] = lr_scheduler[m]\n",
    "    hyperparameters_copy['activation'] = activation[b]\n",
    "    hyperparameters_copy['regularizer'] = regularizer[r]\n",
    "\n",
    "    v_val_loss = []\n",
    "\n",
    "    X_train = np.load(\"../../DATA/s\" + str(i) + \"/X_train_tensor_\" + norm + \".npy\")\n",
    "    y_train = pd.read_csv(\"../../DATA/s\" + str(i) + \"/y_train_tensor_\" + norm + \".csv\")[['individualMRGerm_stac']].individualMRGerm_stac.values\n",
    "\n",
    "    X_val = np.load(\"../../DATA/s\" + str(i) + \"/X_val_tensor_\" + norm + \".npy\")\n",
    "    y_val = pd.read_csv(\"../../DATA/s\" + str(i) + \"/y_val_tensor_\" + norm + \".csv\")[['individualMRGerm_stac']].individualMRGerm_stac.values\n",
    "\n",
    "    reset_keras()\n",
    "\n",
    "    model, hist, early = run_network(\n",
    "        X_train, X_val,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        hyperparameters_copy,\n",
    "        seed\n",
    "    )\n",
    "\n",
    "    v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    metric_dev = np.mean(v_val_loss)\n",
    "    return (metric_dev, k, l, m, b, r, X_train, y_train, X_val, y_val)\n",
    "\n",
    "def myCVGridParallel(hyperparameters, dropout, lr_scheduler, layers, activation, regularizer, seed, split, norm, n_time_steps=14):\n",
    "    bestHyperparameters = {}\n",
    "    bestMetricDev = np.inf\n",
    "\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=3)(\n",
    "        delayed(evaluate_combination)(k, l, m, b, r, hyperparameters, dropout, layers, lr_scheduler, activation, regularizer, seed, split, norm, n_time_steps)\n",
    "        for k in range(len(dropout))\n",
    "        for l in range(len(layers))\n",
    "        for m in range(len(lr_scheduler))\n",
    "        for b in range(len(activation))\n",
    "        for r in range(len(regularizer))\n",
    "    )\n",
    "\n",
    "    for metric_dev, k, l, m, b, r, X_train, y_train, X_val, y_val in results:\n",
    "        if metric_dev < bestMetricDev:\n",
    "            bestMetricDev = metric_dev\n",
    "            bestHyperparameters = {\n",
    "                'dropout': dropout[k],\n",
    "                'layers': layers[l],\n",
    "                'lr_scheduler': lr_scheduler[m],\n",
    "                'activation': activation[b],\n",
    "                'regularizer': regularizer[r],\n",
    "                'X_train': X_train,\n",
    "                'y_train': y_train,\n",
    "                'X_val': X_val,\n",
    "                'y_val': y_val\n",
    "            }\n",
    "\n",
    "    return bestHyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "In the dictionary, hyperparameters related to: data, training, evaluation, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 76, 124, 163, 192, 205]\n",
    "\n",
    "input_shape = 80\n",
    "n_time_steps = 14\n",
    "batch_size = 32\n",
    "n_epochs_max = 1000\n",
    "\n",
    "layer_list = [\n",
    "    [input_shape, 10, 1],  [input_shape, 20, 1], [input_shape, 30, 1], [input_shape, 40, 1], \n",
    "    [input_shape, 50, 1],  [input_shape, 60, 1]\n",
    "]\n",
    "\n",
    "dropout = [0.1, 0.15, 0.2]\n",
    "lr_scheduler = [1e-1]\n",
    "\n",
    "regularizer = [\n",
    "    {'type': 'l2', 'value': 0},\n",
    "    {'type': 'l2', 'value': 0.01}\n",
    "]\n",
    "\n",
    "activation = ['LeakyReLU']\n",
    " \n",
    "norm = \"normPower2\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_time_steps\": n_time_steps,\n",
    "    \"mask_value\": 666,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs_max\": n_epochs_max,\n",
    "    \"monitor\": \"val_loss\",\n",
    "    \"mindelta\": 0,\n",
    "    \"patience\": 15,\n",
    "    \"dropout\": 0.0,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "tab = \"\\t\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "run_model = True\n",
    "debug = True\n",
    "\n",
    "if run_model:\n",
    "    loss_train = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    v_accuracy_test = []\n",
    "    v_specificity = []\n",
    "    v_precision = []\n",
    "    v_recall = []\n",
    "    v_f1score = []\n",
    "    v_roc = []\n",
    "    v_early = []\n",
    "    v_probs = []\n",
    "    results = \"\"\n",
    "\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "\n",
    "    for i in [1, 2, 3]:\n",
    "        init = time.time()\n",
    "        \n",
    "        X_test = np.load(\"../../DATA/s\" +str(i)+ \"/X_test_tensor_\"+norm+\".npy\")\n",
    "        y_test = pd.read_csv(\"../../DATA/s\" +str(i) + \"/y_test_tensor_\"+norm+\".csv\")[['individualMRGerm_stac']].individualMRGerm_stac.values\n",
    "\n",
    "        # GridSearch of hyperparameters \n",
    "        bestHyperparameters = myCVGridParallel(hyperparameters,\n",
    "                                               dropout,\n",
    "                                               lr_scheduler,\n",
    "                                               layer_list,\n",
    "                                               activation,\n",
    "                                               regularizer,\n",
    "                                               seeds[i],\n",
    "                                               \"s\"+str(i),\n",
    "                                               norm)\n",
    "        fin = time.time()\n",
    "        X_train = bestHyperparameters[\"X_train\"]\n",
    "        y_train = bestHyperparameters[\"y_train\"]\n",
    "        X_val = bestHyperparameters[\"X_val\"]\n",
    "        y_val = bestHyperparameters[\"y_val\"]\n",
    "\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_GRU/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "\n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "        hyperparameters = {\n",
    "            'n_time_steps': hyperparameters[\"n_time_steps\"],\n",
    "            'mask_value': hyperparameters[\"mask_value\"],\n",
    "\n",
    "            'batch_size': hyperparameters[\"batch_size\"],\n",
    "            'n_epochs_max': hyperparameters[\"n_epochs_max\"],\n",
    "            'monitor':  hyperparameters[\"monitor\"],\n",
    "            \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "            \"patience\": hyperparameters[\"patience\"],\n",
    "            \"dropout\": bestHyperparameters[\"dropout\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"activation\": bestHyperparameters[\"activation\"],\n",
    "            \"regularizer\": bestHyperparameters[\"regularizer\"],\n",
    "\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "        # --- TRY ON TEST ----------------------------------------------------------------------\n",
    "        reset_keras()\n",
    "\n",
    "        model, hist, early = run_network(\n",
    "            X_train, X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            hyperparameters,\n",
    "            seeds[i-1]\n",
    "        )\n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_train.append(hist.history['loss'])\n",
    "        loss_dev.append(hist.history['val_loss'])\n",
    "\n",
    "        y_pred = model.predict(x=X_test)\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "\n",
    "        # Save y_pred for current split\n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "\n",
    "        # Save model for current split\n",
    "        model_filename = os.path.join(split_directory, f\"model_split_{i}.h5\")\n",
    "        model.save(model_filename)\n",
    "        \n",
    "    \n",
    "        accuracy_test = sklearn.metrics.accuracy_score(y_test, np.round(y_pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, np.round(y_pred)).ravel()\n",
    "        roc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        v_accuracy_test.append(accuracy_test)\n",
    "        v_specificity.append(tn / (tn + fp))\n",
    "        v_precision.append(tp / (tp + fp))\n",
    "        v_recall.append(tp / (tp + fn))\n",
    "        v_f1score.append((2 * v_recall[i-1] * v_precision[i-1]) / (v_recall[i-1] + v_precision[i-1]))\n",
    "        v_roc.append(roc)\n",
    "\n",
    "        if debug:\n",
    "            results = results + tab + \"\\tPositivos bien predichos\" + str(tp) + \"\\n\"\n",
    "            results = results + tab + \"\\tPositivos mal predichos\" + str(fp) + \"\\n\"\n",
    "            results = results + tab + \"\\tNegativos bien predichos\" + str(tn) + \"\\n\"\n",
    "            results = results + tab + \"\\tNegativos mal predichos\" + str(fn) + \"\\n\"\n",
    "        \n",
    "    \n",
    "\n",
    "    # END EXECUTION - SAVE AGGREGATED RESULTS\n",
    "    directory = './Results_GRU'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    def save_to_pickle(data, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    save_to_pickle(bestHyperparameters_bySplit, os.path.join(directory, \"bestHyperparameters_bySplit.pkl\"))\n",
    "    save_to_pickle(y_pred_by_split, os.path.join(directory, \"y_pred_by_split.pkl\"))\n",
    "    \n",
    "    for i, model in enumerate(v_models):\n",
    "        model_filename = os.path.join(directory, f\"model_{i}.h5\")\n",
    "        model.save(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric_line(metric_name, mean_value, deviation_value):\n",
    "    return f\"{metric_name}: {mean_value:.2f} +- {deviation_value:.2f}\\n\"\n",
    "\n",
    "# Calculate the metrics\n",
    "mean_test, deviation_test = calculateKPI(v_accuracy_test)\n",
    "mean_specificity, deviation_specificity = calculateKPI(v_specificity)\n",
    "mean_recall, deviation_recall = calculateKPI(v_recall)\n",
    "mean_f1, deviation_f1 = calculateKPI(v_f1score)\n",
    "mean_precision, deviation_precision = calculateKPI(v_precision)\n",
    "mean_roc, deviation_roc = calculateKPI(v_roc)\n",
    "\n",
    "# Generate the results string\n",
    "results = \"\"\n",
    "results += format_metric_line(\"Test Accuracy\", mean_test, deviation_test)\n",
    "results += format_metric_line(\"Specificity\", mean_specificity, deviation_specificity)\n",
    "results += format_metric_line(\"Sensitivity\", mean_recall, deviation_recall)\n",
    "results += format_metric_line(\"Precision\", mean_precision, deviation_precision)\n",
    "results += format_metric_line(\"F1-score\", mean_f1, deviation_f1)\n",
    "results += format_metric_line(\"ROC-AUC\", mean_roc, deviation_roc)\n",
    "\n",
    "# Final formatted string for all metrics\n",
    "final_results = (\n",
    "    f\"Sensitivity: {mean_recall:.2f} +- {deviation_recall:.2f}\\n\"\n",
    "    f\"Specificity: {mean_specificity:.2f} +- {deviation_specificity:.2f}\\n\"\n",
    "    f\"Precision: {mean_precision:.2f} +- {deviation_precision:.2f}\\n\"\n",
    "    f\"F1-score: {mean_f1:.2f} +- {deviation_f1:.2f}\\n\"\n",
    "    f\"ROC-AUC: {mean_roc:.2f} +- {deviation_roc:.2f}\\n\"\n",
    "    f\"Test Accuracy: {mean_test:.2f} +- {deviation_test:.2f}\\n\"\n",
    ")\n",
    "\n",
    "# Optionally, you can print or log the results\n",
    "print(final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
