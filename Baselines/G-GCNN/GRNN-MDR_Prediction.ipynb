{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99864bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import Modules.architectures as archit\n",
    "import Modules.model as model\n",
    "import Utils.graphTools as graphTools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import dgl as dgl\n",
    "import pickle\n",
    "import datetime\n",
    "from scipy.io import savemat\n",
    "\n",
    "#\\\\\\ Separate functions:\n",
    "from Utils.miscTools import writeVarValues\n",
    "from Utils.miscTools import saveSeed\n",
    "import Utils.graphML as gml\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "numberOfTimeStep = 14\n",
    "carpetas = [\"s1\", \"s2\", \"s3\"]\n",
    "norm = \"normPower2\"\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_kernel(train, sigma):\n",
    "\n",
    "    matrix_train = np.exp(-(train**2)/(2*(sigma**2)))\n",
    "\n",
    "    x = pd.DataFrame(matrix_train)\n",
    "    x = np.round(x, 6)\n",
    "\n",
    "    eigenvalues = np.linalg.eig(x)\n",
    "    matriz = np.array(x)\n",
    "    if not (np.sum(np.abs(eigenvalues[0])>0) == x.shape[0]) and (np.array_equal(matriz, matriz.T)):\n",
    "        print(\"==============================\")\n",
    "        print(\"NO Cumple condici√≥n de kernel\")\n",
    "        print(\"==============================\")\n",
    "\n",
    "    return x\n",
    "\n",
    "def load_data(c, norm):\n",
    "\n",
    "    dtw = pd.read_csv(\"../../E2_Standard-GCN/dtw_matrices/\"+carpetas[c]+\"/tr_AMR_\"+norm+\".csv\")\n",
    "    dtw = dtw.fillna(0)\n",
    "    K = exp_kernel(dtw, 1.5)\n",
    "    K = K-np.eye(K.shape[0])\n",
    "    threshold_val = 0.975\n",
    "    \n",
    "    A = K.copy()\n",
    "    A[np.abs(A) < threshold_val] = 0\n",
    "\n",
    "    # Load raw data\n",
    "    X_train = np.load(\"../../DATA/\" + carpetas[c] + \"/X_train_tensor_\"+norm+\".npy\")\n",
    "    X_train[X_train == 666] = 0\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    X_val = np.load(\"../../DATA/\" + carpetas[c] + \"/X_val_tensor_\"+norm+\".npy\")\n",
    "    X_val[X_val == 666] = 0\n",
    "    print(\"X_val:\", X_val.shape)\n",
    "    X_test = np.load(\"../../DATA/\" + carpetas[c] + \"/X_test_tensor_\"+norm+\".npy\")\n",
    "    X_test[X_test == 666] = 0\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "\n",
    "    y_train = pd.read_csv(\"../../DATA/\" + carpetas[c] + \"/y_train_\"+norm+\".csv\")[['individualMRGerm_stac']]\n",
    "    y_train = y_train.iloc[0:y_train.shape[0]:numberOfTimeStep].reset_index(drop=True)\n",
    "    y_train = torch.tensor(y_train['individualMRGerm_stac'], dtype=torch.float32)\n",
    "    print(\"y_train:\", y_train.shape)\n",
    "    y_val = pd.read_csv(\"../../DATA/\" + carpetas[c] + \"/y_val_tensor_\"+norm+\".csv\")[['individualMRGerm_stac']]\n",
    "    y_val = torch.tensor(y_val['individualMRGerm_stac'], dtype=torch.float32)\n",
    "    print(\"y_val:\", y_val.shape)\n",
    "    y_test = pd.read_csv(\"../../DATA/\" + carpetas[c] + \"/y_test_\"+norm+\".csv\")[['individualMRGerm_stac']]\n",
    "    y_test = y_test.iloc[0:y_test.shape[0]:numberOfTimeStep].reset_index(drop=True)\n",
    "    y_test = torch.tensor(y_test['individualMRGerm_stac'], dtype=torch.float32)\n",
    "    \n",
    "    print(\"y_test:\", y_test.shape)\n",
    "\n",
    "    # Vectorize each of the train/val/test sets\n",
    "    n, dim1, dim2 = X_train.shape\n",
    "    X_train_vec = torch.tensor(X_train.reshape((n, dim1 * dim2)), dtype=torch.float32)\n",
    "#     X_train_vec = X_train_vec.unsqueeze(2) \n",
    "    print(\"X_train_vec:\", X_train_vec.shape)\n",
    "    \n",
    "    n, dim1, dim2 = X_val.shape\n",
    "    X_val_vec = torch.tensor(X_val.reshape((n, dim1 * dim2)), dtype=torch.float32)\n",
    "#     X_val_vec = X_val_vec.unsqueeze(2) \n",
    "    print(\"X_val_vec:\", X_val_vec.shape)\n",
    "    \n",
    "    n, dim1, dim2 = X_test.shape\n",
    "    X_test_vec = torch.tensor(X_test.reshape((n, dim1 * dim2)), dtype=torch.float32)\n",
    "#     X_test_vec = X_test_vec.unsqueeze(2) \n",
    "    print(\"X_test_vec:\", X_test_vec.shape)\n",
    "    \n",
    "    return A, X_train_vec, X_val_vec, X_test_vec, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, target_binary_class, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute accuracy between predicted probabilities and target binary class.\n",
    "    \n",
    "    Args:\n",
    "    - predicted_probabilities: A list or NumPy array of predicted probabilities for the positive class.\n",
    "    - target_binary_class: A list or NumPy array of the actual binary target class (0 or 1).\n",
    "    - threshold: Threshold for converting probabilities to binary predictions (default is 0.5).\n",
    "    \n",
    "    Returns:\n",
    "    - Accuracy: Proportion of correct predictions.\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary predictions based on the threshold\n",
    "    binary_predictions = [1 if p >= threshold else 0 for p in predicted_probabilities]\n",
    "    \n",
    "    # Compare binary predictions with the actual target binary class\n",
    "    correct_predictions = sum(1 for pred, target in zip(binary_predictions, target_binary_class) if pred == target)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(target_binary_class)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27148c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedGCRNNforClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    GatedGCRNNfforClassification: implements the full GCRNN architecture, i.e.\n",
    "    h_t = sigma(\\hat{Q_t}(A(S)*x_t) + \\check{Q_t}(B(S)*h_{t-1}))\n",
    "    y_t = rho(C(S)*h_t)\n",
    "    where:\n",
    "     - h_t, x_t, y_t are the state, input and output variables respectively\n",
    "     - sigma and rho are the state and output nonlinearities\n",
    "     - \\hat{Q_t} and \\check{Q_t} are the input and forget gate operators, which could be time, node or edge gates (or\n",
    "     time+node, time+edge)\n",
    "     - A(S), B(S) and C(S) are the input-to-state, state-to-state and state-to-output graph filters\n",
    "     In the classification version of the Gated GCRNN, y_t is a C-dimensional one-hot vector, where C is the number of classes\n",
    "\n",
    "    Initialization:\n",
    "\n",
    "        GatedGCRNNforClassification(inFeatures, stateFeatures, inputFilterTaps,\n",
    "             stateFilterTaps, stateNonlinearity,\n",
    "             outputNonlinearity,\n",
    "             dimLayersMLP,\n",
    "             GSO,\n",
    "             bias,\n",
    "             time_gating=True,\n",
    "             spatial_gating=None,\n",
    "             mlpType = 'oneMlp'\n",
    "             finalNonlinearity = None,\n",
    "             dimNodeSignals=None, nFilterTaps=None,\n",
    "             nSelectedNodes=None, poolingFunction=None, poolingSize=None, maxN = None)\n",
    "\n",
    "        Input:\n",
    "            inFeatures (int): dimension of the input signal at each node\n",
    "            stateFeatures (int): dimension of the hidden state at each node\n",
    "            inputFilterTaps (int): number of input filter taps\n",
    "            stateFilterTaps (int): number of state filter taps \n",
    "            stateNonlinearity (torch.nn): sigma, state nonlinearity in the GRNN cell\n",
    "            outputNonlinearity (torch.nn): rho, module from torch.nn nonlinear activations\n",
    "            dimLayersMLP (list of int): number of hidden units of the MLP layers\n",
    "            GSO (np.array): graph shift operator\n",
    "            bias (bool): include bias after graph filter on every layer\n",
    "            time_gating (bool): time gating flag, default True\n",
    "            spatial_gating (string): None, 'node' or 'edge'\n",
    "            mlpType (string): either 'oneMlp' or 'multipMLP'; 'multipMLP' corresponds to local MLPs, one per node\n",
    "            finalNonlinearity (torch.nn): nonlinearity to apply to y, if any (e.g. softmax for classification)\n",
    "            dimNodeSignals (list of int): dimension of the signals at each layer of C(S) if it is a GNN\n",
    "            nFilterTaps (list of int): number of filter taps on each layer of C(S) if it is a GNN\n",
    "            nSelectedNodes (list of int): number of nodes to keep after pooling\n",
    "                on each layer of the C(S) if it is a Selection GNN\n",
    "            poolingFunction (nn.Module in Utils.graphML): summarizing function of C(S) if it is a GNN with pooling\n",
    "            poolingSize (list of int): size of the neighborhood to compute the\n",
    "                summary from at each layer if C(S) is a GNN with pooling\n",
    "            maxN (int): maximum number of neighborhood exchanges if C(S) is an Aggregation GNN (default: None)\n",
    "\n",
    "        Output:\n",
    "            nn.Module with a full GRNN architecture, state + output neural networks,\n",
    "            with the above specified characteristics.\n",
    "\n",
    "    Forward call:\n",
    "\n",
    "        GatedGCRNNforClassification(x)\n",
    "\n",
    "        Input:\n",
    "            x (torch.tensor): input data of shape\n",
    "                batchSize x seqLength x dimFeatures x numberNodes\n",
    "\n",
    "        Output:\n",
    "            y (torch.tensor): output data of shape\n",
    "                batchSize x seqLength x dimFeatures x numberNodes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # State GCRNN\n",
    "                 inFeatures, stateFeatures, inputFilterTaps,\n",
    "                 stateFilterTaps, stateNonlinearity,\n",
    "                 # Output NN nonlinearity\n",
    "                 outputNonlinearity,\n",
    "                 # MLP in the end\n",
    "                 dimLayersMLP,\n",
    "                 # Structure\n",
    "                 GSO,\n",
    "                 # Bias\n",
    "                 bias,\n",
    "                 # Gating\n",
    "                 time_gating = True,\n",
    "                 spatial_gating = None,\n",
    "                 # Final nonlinearity, if any, to apply to y\n",
    "                 finalNonlinearity = None,\n",
    "                 # Output NN filtering if output NN is GNN\n",
    "                 dimNodeSignals=None, nFilterTaps=None,\n",
    "                 # Output NN pooling is output NN is GNN with pooling\n",
    "                 nSelectedNodes=None, poolingFunction=None, poolingSize=None, maxN = None):\n",
    "        \n",
    "        # Initialize parent:\n",
    "        super().__init__()\n",
    "        # Check whether the GSO has features or not. After that, always handle\n",
    "        # it as a matrix of dimension E x N x N.\n",
    "        assert len(GSO.shape) == 2 or len(GSO.shape) == 3\n",
    "        if len(GSO.shape) == 2:\n",
    "            assert GSO.shape[0] == GSO.shape[1]\n",
    "            GSO = GSO.reshape([1, GSO.shape[0], GSO.shape[1]]) # 1 x N x N\n",
    "        else:\n",
    "            assert GSO.shape[1] == GSO.shape[2] # E x N x N\n",
    "\n",
    "        # Store the values for the state GRNN (using the notation in the paper):\n",
    "        self.F_i = inFeatures # Input features\n",
    "        self.K_i = inputFilterTaps # Filter taps of input filter\n",
    "        self.F_h = stateFeatures # State features\n",
    "        self.K_h = stateFilterTaps # Filter taps of state filter\n",
    "        self.E = GSO.shape[0] # Number of edge features\n",
    "        self.N = GSO.shape[1] # Number of nodes\n",
    "        self.bias = bias # Boolean\n",
    "        self.time_gating = time_gating # Boolean\n",
    "        self.spatial_gating = spatial_gating # None, \"node\" or \"edge\"\n",
    "        self.S = torch.tensor(GSO).clone()\n",
    "        self.sigma1 = stateNonlinearity\t \n",
    "        # Declare State GCRNN\n",
    "        self.stateGCRNN = GGCRNNCell(self.F_i, self.F_h, self.K_i,\n",
    "                 self.K_h, self.sigma1, self.time_gating, self.spatial_gating,\n",
    "                 self.E, self.bias)\n",
    "        self.stateGCRNN.addGSO(self.S)\n",
    "        # Dimensions of output GNN's  lfully connected layers or of the output MLP\n",
    "        self.dimLayersMLP = dimLayersMLP\n",
    "        # Output neural network nonlinearity\n",
    "        self.sigma2 = outputNonlinearity\n",
    "        # Selection/Aggregation GNN parameters for the output neural network (default None)\n",
    "        self.F_o = dimNodeSignals\n",
    "        self.K_o = nFilterTaps\n",
    "        self.nSelectedNodes = nSelectedNodes\n",
    "        self.rho = poolingFunction\n",
    "        self.alpha = poolingSize\n",
    "        self.maxN = maxN\n",
    "        # Nonlinearity to apply to the output, e.g. softmax for classification (default None)\n",
    "        self.sigma3 = finalNonlinearity    \n",
    "        \n",
    "        #\\\\ If output neural network is MLP:\n",
    "        if dimNodeSignals is None and nFilterTaps is None:\n",
    "            fc = []\n",
    "            if len(self.dimLayersMLP) > 0: # Maybe we don't want to MLP anything\n",
    "                # The first layer has to connect whatever was left of the graph\n",
    "                # signal, flattened.\n",
    "                dimInputMLP = self.N * self.F_h\n",
    "                # (i.e., N nodes, each one described by F_h features,\n",
    "                # which means this will be flattened into a vector of size\n",
    "                    # N x F_h)\n",
    "                fc.append(nn.Linear(dimInputMLP, self.dimLayersMLP[0], bias = self.bias))\n",
    "                for l in range(len(dimLayersMLP)-1):\n",
    "                    # Add the nonlinearity because there's another linear layer\n",
    "                    # coming\n",
    "                    fc.append(self.sigma2())\n",
    "                    # And add the linear layer\n",
    "                    fc.append(nn.Linear(dimLayersMLP[l], dimLayersMLP[l+1],\n",
    "                                        bias = self.bias))   \n",
    "            # And we're done\n",
    "            # Declare output MLP\n",
    "            if self.sigma3 is not None :   \n",
    "                fc.append(self.sigma3())\n",
    "            self.outputNN = nn.Sequential(*fc)\n",
    "            # so we finally have the architecture.\n",
    "\n",
    "\n",
    "    def forward(self, x, h0):\n",
    "        # Now we compute the forward call\n",
    "        H = self.stateGCRNN(x,h0)\n",
    "        h = H.select(1,-1)\n",
    "        if self.F_o is None: # outputNN is MLP\n",
    "            h = h.view(-1,self.F_h*self.N)\n",
    "            y = self.outputNN(h)\n",
    "        else:\n",
    "            y = self.outputNN(h)\n",
    "        return y\n",
    "\n",
    "    def to(self, device):\n",
    "        # Because only the filter taps and the weights are registered as\n",
    "        # parameters, when we do a .to(device) operation it does not move the\n",
    "        # GSOs. So we need to move them ourselves.\n",
    "        # Call the parent .to() method (to move the registered parameters)\n",
    "        super().to(device)\n",
    "        # Move the GSO\n",
    "        self.S = self.S.to(device)\n",
    "        \n",
    "        \n",
    "class GGCRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    GGCRNNCell Creates a gated recurrent layer that computes h_t = sigma(\\hat{Q_t}(A(S)*x_t)\n",
    "    + \\check{Q_t}(B(S)*h_{t-1})), where sigma is a nonlinearity (e.g. tanh), \\hat{Q_t} and\n",
    "    \\check{Q_t} are the input and forget gate operators, A(S) and B(S) are LSI-GFs and h_t and x_t are the state and\n",
    "    input variables respectively. \\hat{Q_t} and \\check{Q_t} can be time, node or edge gates (or time+node, time+edge).\n",
    "\n",
    "    Initialization:\n",
    "\n",
    "        GGCRNNCell(in_features, state_features, in_filter_taps,\n",
    "                    state_filter_taps, sigma, time_gating=True, spatial_gating=None,\n",
    "                    edge_features=1, bias=True)\n",
    "\n",
    "        Inputs:\n",
    "            in_features (int): number of input features (each feature is a graph\n",
    "                signal)\n",
    "            state_features (int): number of state features (each feature is a\n",
    "                graph signal)\n",
    "            in_filter_taps (int): number of filter taps of the input filter\n",
    "            state_filter_taps (int): number of filter taps of the state filter\n",
    "            sigma (torch.nn): state nonlinearity (default tanh)\n",
    "            time_gating (bool) = flag for time gating (default True)\n",
    "            spatial_gating (string) = 'node' or 'edge' gating (default None)\n",
    "            edge_features (int): number of features over each edge\n",
    "            bias (bool): add bias vector (one bias per feature) after graph\n",
    "                filtering\n",
    "\n",
    "        Output:\n",
    "            torch.nn.Module for a gated graph recurrent layer.\n",
    "\n",
    "        Observation: Input filter taps have shape\n",
    "            state_features x edge_features x filter_taps x in_features\n",
    "\n",
    "            State filter taps have shape\n",
    "            state_features x edge_features x filter_taps x state_features\n",
    "\n",
    "    Add graph shift operator:\n",
    "\n",
    "        GGCRNNCell.addGSO(GSO) Before applying the filter, we need to define\n",
    "        the GSO that we are going to use. This allows to change the GSO while\n",
    "        using the same filtering coefficients (as long as the number of edge\n",
    "        features is the same; but the number of nodes can change).\n",
    "\n",
    "        Here we also define the GCRNNs for the input and forget gates, as they\n",
    "        use the same GSO\n",
    "\n",
    "        Inputs:\n",
    "            GSO (torch.tensor): graph shift operator; shape:\n",
    "                edge_features x number_nodes x number_nodes\n",
    "\n",
    "    Forward call:\n",
    "\n",
    "        H = GGCRNNCell(X, h0)\n",
    "\n",
    "        Inputs:\n",
    "            X (torch.tensor): input data; shape:\n",
    "                batch_size x sequence_length x in_features x number_nodes\n",
    "            h0 (torch.tensor): initial hidden state; shape:\n",
    "                batch_size x state_features x number_nodes\n",
    "\n",
    "        Outputs:\n",
    "            H (torch.tensor): output; shape:\n",
    "                batch_size x sequence_length x state_features x number_nodes\n",
    "                \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, G, F, Kin, Kst, sigma=nn.Tanh, time_gating=True, spatial_gating=None, E=1, bias=True):\n",
    "        # G: number of input features\n",
    "        # F: number of state features\n",
    "        # Kin, Kst: number of filter taps\n",
    "        # sigma: nonlinearity (default tanh)\n",
    "        # GSOs will be added later.\n",
    "        # This combines both weight scalars and weight vectors.\n",
    "        # Bias will always be shared and scalar.\n",
    "\n",
    "        # Initialize parent\n",
    "        super().__init__()\n",
    "        # Save parameters:\n",
    "        self.G = G\n",
    "        self.F = F\n",
    "        self.Kin = Kin\n",
    "        self.Kst = Kst\n",
    "        self.E = E\n",
    "        self.S = None  # No GSO assigned yet\n",
    "        # Create parameters:\n",
    "        self.weight_A = nn.parameter.Parameter(torch.Tensor(F, E, Kin, G))\n",
    "        self.weight_B = nn.parameter.Parameter(torch.Tensor(F, E, Kst, F))\n",
    "        self.sigma = sigma\n",
    "        self.time_gating = time_gating\n",
    "        self.spatial_gating = spatial_gating\n",
    "        self.bias_flag = bias\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.Tensor(F, 1))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Taken from _ConvNd initialization of parameters:\n",
    "        stdv = 1. / math.sqrt(self.G * self.Kin)\n",
    "        self.weight_A.data.uniform_(-stdv, stdv)\n",
    "        self.weight_B.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def addGSO(self, S):\n",
    "        # Every S has 3 dimensions.\n",
    "        assert len(S.shape) == 3\n",
    "        # S is of shape E x N x N\n",
    "        assert S.shape[0] == self.E\n",
    "        self.N = S.shape[1]\n",
    "        assert S.shape[2] == self.N\n",
    "        self.S = S.clone()\n",
    "\n",
    "    def forward(self, X, h0):\n",
    "        # X is of shape: batchSize x seqLen x dimInFeatures x numberNodesIn\n",
    "        # h0 is of shape: batchSize x dimStFeatures x numberNodesIn\n",
    "        assert h0.shape[0] == X.shape[0]\n",
    "        B = X.shape[0]\n",
    "        T = X.shape[1]\n",
    "        F_in = X.shape[2]\n",
    "\n",
    "        # Defaults when there is no time gating\n",
    "        in_gate = torch.ones([B, 1]);\n",
    "        forget_gate = torch.ones([B, 1]);\n",
    "\n",
    "        # Compute the GCRNN cell output\n",
    "        H = torch.empty(0)  # state sequence\n",
    "        h = h0  # current state\n",
    "        for i in range(T):\n",
    "            # Slice input at time t\n",
    "            x = torch.narrow(X, 1, i, 1)\n",
    "            x = x.view(B, F_in, self.N)\n",
    "           \n",
    "            # Just time gating (if True)\n",
    "            # Apply filters\n",
    "            h = in_gate.view(-1, 1, 1).repeat(1, self.F, self.N) * LSIGF(self.weight_A, self.S, x, self.bias) + \\\n",
    "                forget_gate.view(-1, 1, 1).repeat(1, self.F, self.N) * LSIGF(self.weight_B, self.S, h, self.bias)\n",
    "            # Apply hyperbolic tangent\n",
    "            h = self.sigma(h)\n",
    "\n",
    "            h_unsq = h.unsqueeze(1)  # re-create sequence dimension\n",
    "            # concatenate last state, h, with memory sequence H\n",
    "            H = torch.cat([H, h_unsq], 1)\n",
    "        return H\n",
    "    \n",
    "    \n",
    "def LSIGF(h, S, x, b=None):\n",
    "    \"\"\"\n",
    "    LSIGF(filter_taps, GSO, input, bias=None) Computes the output of a linear\n",
    "        shift-invariant graph filter on input and then adds bias.\n",
    "\n",
    "    Denote as G the number of input features, F the number of output features,\n",
    "    E the number of edge features, K the number of filter taps, N the number of\n",
    "    nodes, S_{e} in R^{N x N} the GSO for edge feature e, x in R^{G x N} the\n",
    "    input data where x_{g} in R^{N} is the graph signal representing feature\n",
    "    g, and b in R^{F x N} the bias vector, with b_{f} in R^{N} representing the\n",
    "    bias for feature f.\n",
    "\n",
    "    Then, the LSI-GF is computed as\n",
    "        y_{f} = \\sum_{e=1}^{E}\n",
    "                    \\sum_{k=0}^{K-1}\n",
    "                    \\sum_{g=1}^{G}\n",
    "                        [h_{f,g,e}]_{k} S_{e}^{k} x_{g}\n",
    "                + b_{f}\n",
    "    for f = 1, ..., F.\n",
    "\n",
    "    Inputs:\n",
    "        filter_taps (torch.tensor): array of filter taps; shape:\n",
    "            output_features x edge_features x filter_taps x input_features\n",
    "        GSO (torch.tensor): graph shift operator; shape:\n",
    "            edge_features x number_nodes x number_nodes\n",
    "        input (torch.tensor): input signal; shape:\n",
    "            batch_size x input_features x number_nodes\n",
    "        bias (torch.tensor): shape: output_features x number_nodes\n",
    "            if the same bias is to be applied to all nodes, set number_nodes = 1\n",
    "            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N}\n",
    "\n",
    "    Outputs:\n",
    "        output: filtered signals; shape:\n",
    "            batch_size x output_features x number_nodes\n",
    "    \"\"\"\n",
    "    # The basic idea of what follows is to start reshaping the input and the\n",
    "    # GSO so the filter coefficients go just as a very plain and simple\n",
    "    # linear operation, so that all the derivatives and stuff on them can be\n",
    "    # easily computed.\n",
    "\n",
    "    # h is output_features x edge_weights x filter_taps x input_features\n",
    "    # S is edge_weighs x number_nodes x number_nodes\n",
    "    # x is batch_size x input_features x number_nodes\n",
    "    # b is output_features x number_nodes\n",
    "    # Output:\n",
    "    # y is batch_size x output_features x number_nodes\n",
    "\n",
    "    # Get the parameter numbers:\n",
    "    F = h.shape[0]\n",
    "    E = h.shape[1]\n",
    "    K = h.shape[2]\n",
    "    G = h.shape[3]\n",
    "    assert S.shape[0] == E\n",
    "    N = S.shape[1]\n",
    "    assert S.shape[2] == N\n",
    "    B = x.shape[0]\n",
    "    assert x.shape[1] == G\n",
    "    assert x.shape[2] == N\n",
    "    # Or, in the notation we've been using:\n",
    "    # h in F x E x K x G\n",
    "    # S in E x N x N\n",
    "    # x in B x G x N\n",
    "    # b in F x N\n",
    "    # y in B x F x N\n",
    "\n",
    "    # Now, we have x in B x G x N and S in E x N x N, and we want to come up\n",
    "    # with matrix multiplication that yields z = x * S with shape\n",
    "    # B x E x K x G x N.\n",
    "    # For this, we first add the corresponding dimensions\n",
    "    x = x.reshape([B, 1, G, N])\n",
    "    S = S.reshape([1, E, N, N])\n",
    "    z = x.reshape([B, 1, 1, G, N]).repeat(1, E, 1, 1, 1) # This is for k = 0\n",
    "    # We need to repeat along the E dimension, because for k=0, S_{e} = I for\n",
    "    # all e, and therefore, the same signal values have to be used along all\n",
    "    # edge feature dimensions.\n",
    "    for k in range(1,K):\n",
    "        x = x.to(S.dtype)\n",
    "        x = torch.matmul(x, S) # B x E x G x N\n",
    "        xS = x.reshape([B, E, 1, G, N]) # B x E x 1 x G x N\n",
    "        z = torch.cat((z, xS), dim = 2) # B x E x k x G x N\n",
    "    # This output z is of size B x E x K x G x N\n",
    "    # Now we have the x*S_{e}^{k} product, and we need to multiply with the\n",
    "    # filter taps.\n",
    "    # We multiply z on the left, and h on the right, the output is to be\n",
    "    # B x N x F (the multiplication is not along the N dimension), so we reshape\n",
    "    # z to be B x N x E x K x G and reshape it to B x N x EKG (remember we\n",
    "    # always reshape the last dimensions), and then make h be E x K x G x F and\n",
    "    # reshape it to EKG x F, and then multiply\n",
    "    z = z.to(h.dtype)\n",
    "    y = torch.matmul(z.permute(0, 4, 1, 2, 3).reshape([B, N, E*K*G]),\n",
    "                     h.reshape([F, E*K*G]).permute(1, 0)).permute(0, 2, 1)\n",
    "    # And permute againt to bring it from B x N x F to B x F x N.\n",
    "    # Finally, add the bias\n",
    "    if b is not None:\n",
    "        y = y + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph(A):\n",
    "    # ====> CREATE GRAPH BY ADJACENCY MATRIX\n",
    "    graphType = 'adjacency'\n",
    "    graphOptions = {}\n",
    "    graphOptions['adjacencyMatrix'] = A.values\n",
    "    G = graphTools.Graph(graphType, A.shape[0], graphOptions)\n",
    "    G.computeGFT()\n",
    "    S = G.S/np.abs(np.max(np.diag(G.E)))\n",
    "    S = np.expand_dims(S,axis=0)\n",
    "    order = np.arange(G.N)\n",
    "    ## END CREATE GRAPH <====================\n",
    "    nNodes = A.shape[0]\n",
    "    \n",
    "    return S, nNodes, order\n",
    "\n",
    "\n",
    "def createModel(input_arguments, params): \n",
    "    ################\n",
    "    # ARCHITECTURE #\n",
    "    ################\n",
    "\n",
    "    model = GatedGCRNNforClassification(input_arguments['inFeatures'],\n",
    "                                     input_arguments['stateFeatures'],\n",
    "                                     input_arguments['inputFilterTaps'],\n",
    "                                     input_arguments['stateFilterTaps'],\n",
    "                                     input_arguments['stateNonlinearity'],\n",
    "                                     input_arguments['outputNonlinearity'],\n",
    "                                     input_arguments['dimLayersMLP'],\n",
    "                                     params['S'],\n",
    "                                     input_arguments['bias'],\n",
    "                                     input_arguments['time_gating'],\n",
    "                                     input_arguments['spatial_gating'])\n",
    "    # This is necessary to move all the learnable parameters to be\n",
    "    # stored in the device (mostly, if it's a GPU)\n",
    "    model.to(params['device'])\n",
    "\n",
    "    #############\n",
    "    # OPTIMIZER #\n",
    "    #############\n",
    "\n",
    "    if params['trainer'] == 'ADAM':\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                                lr = params['learningRate'], weight_decay=params['weight_decay'],\n",
    "                                betas = (params['beta1'],params['beta2']))\n",
    "        \n",
    "    elif params['trainer'] == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['learningRate'])\n",
    "        \n",
    "    elif params['trainer'] == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(),\n",
    "                                  lr = params['learningRate'], alpha = params['beta1'])\n",
    "    \n",
    "\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def getbatchIndex(batchSize, nTrain):\n",
    "    ###########################################\n",
    "    # DATA INPUT (pick up on data parameters) #\n",
    "    ###########################################\n",
    "\n",
    "    # nTrain: size of the training set\n",
    "\n",
    "    # Number of batches: If the desired number of batches does not split the\n",
    "    # dataset evenly, we reduce the size of the last batch (the number of\n",
    "    # samples in the last batch).\n",
    "    # The variable batchSize is a list of length nBatches (number of batches),\n",
    "    # where each element of the list is a number indicating the size of the\n",
    "    # corresponding batch.\n",
    "    if nTrain < batchSize:\n",
    "        nBatches = 1\n",
    "        batchSize = [nTrain]\n",
    "    elif nTrain % batchSize != 0:\n",
    "        nBatches = np.ceil(nTrain/batchSize).astype(np.int64)\n",
    "        batchSize = [batchSize] * nBatches\n",
    "        # If the sum of all batches so far is not the total number of graphs,\n",
    "        # start taking away samples from the last batch (remember that we used\n",
    "        # ceiling, so we are overshooting with the estimated number of batches)\n",
    "        while sum(batchSize) != nTrain:\n",
    "            batchSize[-1] -= 1\n",
    "    # If they fit evenly, then just do so.\n",
    "    else:\n",
    "        nBatches = np.int(nTrain/batchSize)\n",
    "        batchSize = [batchSize] * nBatches\n",
    "    # batchIndex is used to determine the first and last element of each batch.\n",
    "    # If batchSize is, for example [20,20,20] meaning that there are three\n",
    "    # batches of size 20 each, then cumsum will give [20,40,60] which determines\n",
    "    # the last index of each batch: up to 20, from 20 to 40, and from 40 to 60.\n",
    "    # We add the 0 at the beginning so that batchIndex[b]:batchIndex[b+1] gives\n",
    "    # the right samples for batch b.\n",
    "    batchIndex = np.cumsum(batchSize).tolist()\n",
    "    batchIndex = [0] + batchIndex\n",
    "    \n",
    "    return batchIndex, nBatches, batchSize\n",
    "\n",
    "\n",
    "def training_Phase(X_train_vec, y_train, X_val_vec, y_val, params, modelName):\n",
    "\n",
    "    bestMetricDev = float('inf')\n",
    "    bestHyperparameters = {'lr':-1, 'weight_decay':-1, 'trainer': -1}\n",
    "    early_stopping_patience = params['early_stopping_patience']\n",
    "    nTrain = X_train_vec.shape[0]\n",
    "    nValid = X_val_vec.shape[0]\n",
    "    weights_to_ADAM = params[\"weight_decay\"]\n",
    "\n",
    "    for lr in range(len(params[\"learningRate\"])):\n",
    "        for t in range(len(params[\"trainer\"])):\n",
    "            if params[\"trainer\"][t] == \"ADAM\":\n",
    "                params[\"weight_decay\"] = weights_to_ADAM\n",
    "            else:\n",
    "                params[\"weight_decay\"] = [0]\n",
    "                \n",
    "            for w in range(len(params[\"weight_decay\"])):\n",
    "                params_aux = params.copy()\n",
    "                params_aux[\"learningRate\"] = params[\"learningRate\"][lr]\n",
    "                params_aux[\"weight_decay\"] = params[\"weight_decay\"][w]\n",
    "                params_aux[\"trainer\"] = params[\"trainer\"][t]\n",
    "                print(\"learningRate: \", params_aux[\"learningRate\"], \"- weight_decay:\", params_aux[\"weight_decay\"], \"- trainer:\", params_aux[\"trainer\"])\n",
    "\n",
    "                model, optimizer = createModel(input_arguments, params_aux)\n",
    "\n",
    "                batchSize = params['batchSize']\n",
    "                batchIndex, nBatches, batchSize = getbatchIndex(batchSize, nTrain)\n",
    "\n",
    "                # Model tracking\n",
    "                # lossTrain_arr = []\n",
    "                # accTrain_arr = []\n",
    "                # lossVal_arr = []\n",
    "                # accVal_arr = []\n",
    "                best_validation_loss = float('inf')\n",
    "                counter_early_stop = 0\n",
    "\n",
    "                for epoch in range(nEpochs):\n",
    "\n",
    "                    # Randomize dataset for each epoch\n",
    "                    randomPermutation = np.random.permutation(nTrain)\n",
    "                    # Convert a numpy.array of numpy.int into a list of actual int.\n",
    "                    idxEpoch = [int(i) for i in randomPermutation]\n",
    "\n",
    "                    # Initialize counter\n",
    "                    for batch in range(nBatches):\n",
    "                        # Extract the adequate batch\n",
    "                        thisBatchIndices = idxEpoch[batchIndex[batch] : batchIndex[batch+1]]\n",
    "                        # Get the samples\n",
    "                        xTrain, yTrain = X_train_vec[thisBatchIndices], y_train[thisBatchIndices]\n",
    "                        xTrain = xTrain.view(batchSize[batch],seqLen,-1)\n",
    "                        yTrain = yTrain.view(batchSize[batch],-1)\n",
    "\n",
    "                        # Set the ordering\n",
    "                        xTrainOrdered = xTrain[:,:,params['order']] # B x F x N\n",
    "\n",
    "                        # Check if it is an RNN to add sequence dimension\n",
    "                        if 'RNN' in modelName or 'rnn' in modelName or 'Rnn' in modelName:\n",
    "                            xTrainOrdered = xTrainOrdered.unsqueeze(2) # To account for just F=1 feature\n",
    "                            yTrainModel = yTrain.unsqueeze(2) # To account for just F=1 feature\n",
    "                        else:\n",
    "                            xTrainOrdered = xTrainOrdered.view(batchSize[batch]*seqLen,1,-1)\n",
    "                            yTrainModel = yTrain.view(batchSize[batch]*seqLen,1,-1)\n",
    "\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if 'GCRNN' in modelName or 'gcrnn' in modelName or 'GCRnn' in modelName:\n",
    "                            # Obtain the output of the GCRNN\n",
    "                            h0 = torch.zeros(batchSize[batch],F1,xTrainOrdered.shape[3])\n",
    "                            yHatTrain = model(xTrainOrdered, h0)\n",
    "\n",
    "                        elif 'RNN' in modelName or 'rnn' in modelName or 'Rnn' in modelName:\n",
    "                            # Obtain the output of the GCRNN\n",
    "                            h0 = torch.zeros(batchSize[batch],rnnStateFeat)\n",
    "                            c0 = h0\n",
    "                            yHatTrain = model(xTrainOrdered, h0, c0)\n",
    "                        else:\n",
    "                            # Obtain the output of the GNN\n",
    "                            yHatTrain = model(xTrainOrdered)\n",
    "                            yHatTrain = yHatTrain.unsqueeze(1)\n",
    "\n",
    "                        # Compute loss\n",
    "                        m = nn.Sigmoid()\n",
    "                        yHatTrain = m(yHatTrain)\n",
    "                        if nClass == 2:\n",
    "                            yHatTrain = yHatTrain[:,1]\n",
    "                        else:\n",
    "                            yHatTrain = yHatTrain.view(-1)\n",
    "                        yTrainModel = yTrainModel.view(-1)\n",
    "                        lossValueTrain = lossFunction(yHatTrain,yTrainModel)\n",
    "\n",
    "                        # Compute gradients\n",
    "                        lossValueTrain.backward()\n",
    "\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            accTrain = evaluate(yHatTrain, yTrainModel)\n",
    "\n",
    "                        #\\\\\\\\\\\\\\\n",
    "                        #\\\\\\ VALIDATION\n",
    "                        xValid, yValid = X_val_vec, y_val\n",
    "                        xValid = xValid.view(nValid,seqLen,-1)\n",
    "                        yValid = yValid.view(nValid,-1)\n",
    "\n",
    "                        # Set the ordering\n",
    "                        xValidOrdered = xValid[:,:,order] # BxFxN\n",
    "                        xValidOrdered = xValidOrdered.unsqueeze(2)\n",
    "                        yValidModel = yValid.unsqueeze(2)\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            # Obtain the output of the GNN\n",
    "                            if 'GCRNN' in modelName or 'gcrnn' in modelName or 'GCRnn' in modelName:\n",
    "                                h0v = torch.zeros(nValid,F1,xValidOrdered.shape[3])\n",
    "                                yHatValid = model(xValidOrdered,h0v)\n",
    "                            elif 'RNN' in modelName or 'rnn' in modelName or 'Rnn' in modelName:\n",
    "                                # Obtain the output of the GCRNN\n",
    "                                h0v = torch.zeros(nValid,rnnStateFeat)\n",
    "                                c0v = h0\n",
    "                                yHatValid = model(xValidOrdered,h0v,c0v)\n",
    "                            else:\n",
    "                                yHatValid = model(xValidOrdered)\n",
    "                                yHatValid = yHatValid.unsqueeze(1)\n",
    "\n",
    "                            # Compute loss\n",
    "                            m = nn.Sigmoid()\n",
    "                            yHatValid = m(yHatValid)\n",
    "\n",
    "                            if nClass == 2:\n",
    "                                yHatValid = yHatValid[:,1]\n",
    "                            else:\n",
    "                                yHatValid = yHatValid.view(-1)\n",
    "                            yValidModel = yValidModel.view(-1)\n",
    "                            lossValueValid = lossFunction(yHatValid,yValidModel)\n",
    "\n",
    "                    # Early stopping\n",
    "                    if lossValueValid < best_validation_loss:\n",
    "                        best_validation_loss = lossValueValid\n",
    "                        counter_early_stop = 0\n",
    "                    else:\n",
    "                        counter_early_stop += 1\n",
    "\n",
    "                    if counter_early_stop >= early_stopping_patience:\n",
    "                        print(f'Early stopping en la epoch {epoch}')\n",
    "                        break\n",
    "\n",
    "                    #\\\\\\\\\\\\\\\n",
    "                    #\\\\\\ END OF EPOCH:\n",
    "                    #\\\\\\\\\\\\\\\n",
    "\n",
    "                if best_validation_loss < bestMetricDev:\n",
    "                    print(\"\\t\\tCambio the best\", bestMetricDev, \"por metric dev:\", best_validation_loss)\n",
    "                    bestMetricDev = best_validation_loss\n",
    "                    bestHyperparameters['lr'] = lr\n",
    "                    bestHyperparameters['weight_decay'] = w\n",
    "                    bestHyperparameters['trainer'] = t\n",
    "                \n",
    "    return bestHyperparameters\n",
    "\n",
    "\n",
    "def best_Model(X_train_vec, y_train, X_val_vec, y_val, params, bestParams, modelName):\n",
    "\n",
    "    nTrain = X_train_vec.shape[0]\n",
    "    nValid = X_val_vec.shape[0]\n",
    "    finalParams = params.copy()\n",
    "    finalParams[\"learningRate\"] = params[\"learningRate\"][bestParams[\"lr\"]]\n",
    "    finalParams[\"weight_decay\"] = params[\"weight_decay\"][bestParams[\"weight_decay\"]]\n",
    "    finalParams[\"trainer\"] = params[\"trainer\"][bestParams[\"trainer\"]]\n",
    "    early_stopping_patience = finalParams['early_stopping_patience']\n",
    "\n",
    "    model, optimizer = createModel(input_arguments, finalParams)\n",
    "\n",
    "    batchSize = finalParams['batchSize']\n",
    "    batchIndex, nBatches, batchSize = getbatchIndex(batchSize, nTrain)\n",
    "\n",
    "    best_validation_loss = float('inf')\n",
    "    counter_early_stop = 0\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "\n",
    "        # Randomize dataset for each epoch\n",
    "        randomPermutation = np.random.permutation(nTrain)\n",
    "        # Convert a numpy.array of numpy.int into a list of actual int.\n",
    "        idxEpoch = [int(i) for i in randomPermutation]\n",
    "\n",
    "        # Initialize counter\n",
    "        for batch in range(nBatches):\n",
    "            # Extract the adequate batch\n",
    "            thisBatchIndices = idxEpoch[batchIndex[batch] : batchIndex[batch+1]]\n",
    "            # Get the samples\n",
    "            xTrain, yTrain = X_train_vec[thisBatchIndices], y_train[thisBatchIndices]\n",
    "            xTrain = xTrain.view(batchSize[batch],seqLen,-1)\n",
    "            yTrain = yTrain.view(batchSize[batch],-1)\n",
    "\n",
    "            # Set the ordering\n",
    "            xTrainOrdered = xTrain[:,:,finalParams['order']] # B x F x N\n",
    "\n",
    "            # Check if it is an RNN to add sequence dimension\n",
    "            if 'RNN' in modelName or 'rnn' in modelName or 'Rnn' in modelName:\n",
    "                xTrainOrdered = xTrainOrdered.unsqueeze(2) # To account for just F=1 feature\n",
    "                yTrainModel = yTrain.unsqueeze(2) # To account for just F=1 feature\n",
    "            else:\n",
    "                xTrainOrdered = xTrainOrdered.view(batchSize[batch]*seqLen,1,-1)\n",
    "                yTrainModel = yTrain.view(batchSize[batch]*seqLen,1,-1)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if 'GCRNN' in modelName or 'gcrnn' in modelName or 'GCRnn' in modelName:\n",
    "                # Obtain the output of the GCRNN\n",
    "                h0 = torch.zeros(batchSize[batch],F1,xTrainOrdered.shape[3])\n",
    "                yHatTrain = model(xTrainOrdered, h0)\n",
    "\n",
    "            elif 'RNN' in modelName or 'rnn' in modelName or 'Rnn' in modelName:\n",
    "                # Obtain the output of the GCRNN\n",
    "                h0 = torch.zeros(batchSize[batch],rnnStateFeat)\n",
    "                c0 = h0\n",
    "                yHatTrain = model(xTrainOrdered, h0, c0)\n",
    "            else:\n",
    "                # Obtain the output of the GNN\n",
    "                yHatTrain = model(xTrainOrdered)\n",
    "                yHatTrain = yHatTrain.unsqueeze(1)\n",
    "\n",
    "            # Compute loss\n",
    "            m = nn.Sigmoid()\n",
    "            yHatTrain = m(yHatTrain)\n",
    "            if nClass == 2:\n",
    "                yHatTrain = yHatTrain[:,1]\n",
    "            else:\n",
    "                yHatTrain = yHatTrain.view(-1)\n",
    "            yTrainModel = yTrainModel.view(-1)\n",
    "            lossValueTrain = lossFunction(yHatTrain,yTrainModel)\n",
    "\n",
    "            # Compute gradients\n",
    "            lossValueTrain.backward()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            #\\\\\\\\\\\\\\\n",
    "            #\\\\\\ VALIDATION\n",
    "            xValid, yValid = X_val_vec, y_val\n",
    "            xValid = xValid.view(nValid,seqLen,-1)\n",
    "            yValid = yValid.view(nValid,-1)\n",
    "\n",
    "            # Set the ordering\n",
    "            xValidOrdered = xValid[:,:,order] # BxFxN\n",
    "            xValidOrdered = xValidOrdered.unsqueeze(2)\n",
    "            yValidModel = yValid.unsqueeze(2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Obtain the output of the GNN\n",
    "                if 'GCRNN' in modelName or 'gcrnn' in modelName or 'GCRnn' in modelName:\n",
    "                    h0v = torch.zeros(nValid,F1,xValidOrdered.shape[3])\n",
    "                    yHatValid = model(xValidOrdered,h0v)\n",
    "                elif 'RNN' in modelName or 'rnn' in modelName or 'Rnn' in modelName:\n",
    "                    # Obtain the output of the GCRNN\n",
    "                    h0v = torch.zeros(nValid,rnnStateFeat)\n",
    "                    c0v = h0\n",
    "                    yHatValid = model(xValidOrdered,h0v,c0v)\n",
    "                else:\n",
    "                    yHatValid = model(xValidOrdered)\n",
    "                    yHatValid = yHatValid.unsqueeze(1)\n",
    "\n",
    "                # Compute loss\n",
    "                m = nn.Sigmoid()\n",
    "                yHatValid = m(yHatValid)\n",
    "\n",
    "                if nClass == 2:\n",
    "                    yHatValid = yHatValid[:,1]\n",
    "                else:\n",
    "                    yHatValid = yHatValid.view(-1)\n",
    "                yValidModel = yValidModel.view(-1)\n",
    "                lossValueValid = lossFunction(yHatValid,yValidModel)\n",
    "\n",
    "        # Early stopping\n",
    "        if lossValueValid < best_validation_loss:\n",
    "            best_validation_loss = lossValueValid\n",
    "            counter_early_stop = 0\n",
    "        else:\n",
    "            counter_early_stop += 1\n",
    "\n",
    "        if counter_early_stop >= early_stopping_patience:\n",
    "            print(f'Early stopping en la epoch {epoch}')\n",
    "            break\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "def test_Phase(model, X_test_vec, y_test, nNodes, results, modelName):\n",
    "    nTest = X_test_vec.shape[0]\n",
    "    xTest, yTest = X_test_vec, y_test\n",
    "    xTest = xTest.view(nTest, seqLen, -1)\n",
    "    yTest = yTest.view(nTest, -1)\n",
    "\n",
    "    # Update order and adapt dimensions\n",
    "    xTestOrdered = xTest[:, :, order]\n",
    "    if 'RNN' in modelName:\n",
    "        xTestOrdered = xTestOrdered.unsqueeze(2)\n",
    "    else:\n",
    "        xTestOrdered = xTestOrdered.view(nTest, K, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Process the samples\n",
    "        if 'GCRNN' in modelName:\n",
    "            h0t = torch.zeros(nTest, F1, nNodes).to(xTestOrdered.device)  # Ensure device compatibility\n",
    "            yHatTest = model(xTestOrdered, h0t)\n",
    "        elif 'RNN' in modelName:\n",
    "            h0t = torch.zeros(nTest, rnnStateFeat).to(xTestOrdered.device)  # Ensure device compatibility\n",
    "            c0t = h0t\n",
    "            yHatTest = model(xTestOrdered, h0t, c0t)\n",
    "        else:\n",
    "            yHatTest = model(xTestOrdered)\n",
    "\n",
    "        m = nn.Sigmoid()\n",
    "        yHatTest = m(yHatTest)\n",
    "        yHatTest = yHatTest[:, 1]\n",
    "        thisAccBest = evaluate(yHatTest, yTest)\n",
    "\n",
    "\n",
    "    # Convert predictions to binary\n",
    "    yHatTest = yHatTest.round()\n",
    "\n",
    "    # Compute ROC AUC score\n",
    "    roc_auc = roc_auc_score(yTest.detach().numpy(), yHatTest.detach().numpy())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(yTest.detach().numpy(), yHatTest.detach().numpy()).ravel()\n",
    "\n",
    "    # Compute sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(yTest.detach().numpy(), yHatTest.detach().numpy())\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(yTest.detach().numpy(), yHatTest.detach().numpy())\n",
    "\n",
    "    # Store results\n",
    "    results['roc_auc'].append(roc_auc)\n",
    "    results['sensitivity'].append(sensitivity)\n",
    "    results['specificity'].append(specificity)\n",
    "    results['f1_score'].append(f1)\n",
    "    results['accuracy'].append(accuracy)\n",
    "\n",
    "    # Print results\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05c778",
   "metadata": {},
   "source": [
    "## Build model: GCRNN + MLP (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%##################################################################\n",
    "#                                                                   #\n",
    "#                    SETTING PARAMETERS                             #\n",
    "#                                                                   #\n",
    "#####################################################################\n",
    "\n",
    "modelName = \"GCRNN\"\n",
    "\n",
    "# Linear layer\n",
    "seqLen = 14\n",
    "K = seqLen\n",
    "F1 = 14 # Number of state features for the GCRNN architectures \n",
    "K1 = 2 # Number of filter taps for all filters\n",
    "rnnStateFeat = 2 # Number of state features for the RNN architecture\n",
    "nClass = 2 # Number of class\n",
    "\n",
    "input_arguments = {} \n",
    "input_arguments['name'] = 'GCRNN' # Name of the architecture\n",
    "#\\\\\\ Architecture parameters\n",
    "input_arguments['inFeatures'] = 1 \n",
    "input_arguments['stateFeatures'] = F1 \n",
    "input_arguments['inputFilterTaps'] = K1 \n",
    "input_arguments['stateFilterTaps'] = K1 \n",
    "input_arguments['stateNonlinearity'] = torch.tanh\n",
    "input_arguments['outputNonlinearity'] = nn.ReLU\n",
    "input_arguments['dimLayersMLP'] = [nClass] \n",
    "input_arguments['bias'] = True\n",
    "input_arguments['time_gating'] = False\n",
    "input_arguments['spatial_gating'] = None\n",
    "\n",
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "#\\\\\\ Individual model training options\n",
    "trainer = ['ADAM','SGD', 'RMSprop'] # Options: 'SGD', 'ADAM', 'RMSprop'\n",
    "# learningRate = [1e-3, 0.5e-3, 1e-2, 0.5e-2, 1e-1] # In all options\n",
    "learningRate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "beta1 = 0.9 # beta1 if 'ADAM', alpha if 'RMSprop'\n",
    "beta2 = 0.999 # ADAM option only\n",
    "weight_decay = [1e-7, 1e-5, 1e-3, 1e-1, 0]\n",
    "# weight_decay = [0, 1e-8, 1e-5, 1e-3]\n",
    "\n",
    "#\\\\\\ Loss function choice\n",
    "lossFunction = nn.BCELoss()\n",
    "\n",
    "#\\\\\\ Overall training options\n",
    "nEpochs = 20 # Number of epochs\n",
    "\n",
    "\n",
    "params_model = {'learningRate': learningRate, \n",
    "          'beta1': beta1, 'beta2': beta2,\n",
    "          'weight_decay': weight_decay,\n",
    "          # Others params\n",
    "          'device':device, 'trainer': trainer, \n",
    "          'batchSize': 16, 'early_stopping_patience': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd117da9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = {'roc_auc':[], 'sensitivity':[], 'specificity':[], 'f1_score':[], 'accuracy':[]}\n",
    "bestHyperparameters = {}\n",
    "\n",
    "for c in range(len(carpetas)):\n",
    "    A, X_train_vec, X_val_vec, X_test_vec, y_train, y_val, y_test = load_data(c, norm)\n",
    "    S, nNodes, order = createGraph(A)\n",
    "    \n",
    "    params_model['S'] = S\n",
    "    params_model['order'] = order\n",
    "\n",
    "    bestParamsToCreateModel = training_Phase(X_train_vec, y_train, X_val_vec, y_val, params_model, modelName)\n",
    "    \n",
    "    params_bestModel = {'learningRate': learningRate, \n",
    "          'beta1': beta1, 'beta2': beta2,\n",
    "          'weight_decay': weight_decay,\n",
    "          # Others params\n",
    "          'device':device, 'trainer': trainer, \n",
    "          'batchSize': 16, 'early_stopping_patience': 50,\n",
    "          'S': S, 'order': order}\n",
    "    \n",
    "    model = best_Model(X_train_vec, y_train, X_val_vec, y_val, params_bestModel, bestParamsToCreateModel, modelName)\n",
    "    results = test_Phase(model, X_test_vec, y_test, nNodes, results, modelName)\n",
    "    bestHyperparameters[carpetas[c]] = bestParamsToCreateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4762dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(results.keys())\n",
    "for c in range(len(carpetas)):\n",
    "    print(\"================= SPLIT \" + str(carpetas[c]) + \" ===================\")\n",
    "    print(keys[0] + \": \" + str(np.round(results[keys[0]][c]*100,3)))\n",
    "    print(keys[1] + \": \" + str(np.round(results[keys[1]][c]*100,3)))\n",
    "    print(keys[2] + \": \" + str(np.round(results[keys[2]][c]*100,3)))\n",
    "    \n",
    "print()\n",
    "keys = list(results.keys())\n",
    "for i in range(len(results.keys())):\n",
    "    average = np.mean(results[keys[i]])\n",
    "    std = np.std(results[keys[i]])\n",
    "    print(keys[i] + \": \" + str(np.round(average*100,2)) + \" +- \" + str(np.round(std*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def saveBestHyperparameters(best_hyperparameters, nombre):\n",
    "    with open(nombre, 'w') as archivo:\n",
    "        json.dump(best_hyperparameters, archivo, indent=4)\n",
    "        \n",
    "def loadBestHyperparameters(nombre):\n",
    "    with open(nombre, 'r') as archivo:\n",
    "        hyperparameteres = json.load(archivo)\n",
    "        \n",
    "    return hyperparameteres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ce7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestHyperparameters(bestHyperparameters, \"GRNN_bestHyperparameters\")\n",
    "saveBestHyperparameters(results, \"GRNN_bestResults\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
